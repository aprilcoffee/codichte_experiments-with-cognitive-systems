{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gertrude Stein on NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **prepositions** are usually wrong\n",
    "2.  **articles** are delicate and varied items\n",
    "3. **adjectives** are not interesting\n",
    "4. **nouns** are not interesting\n",
    "5. **verbs** are in motion\n",
    "6. & **adverbs** moves with them\n",
    "7. **pronomis** are moving in a very large space of possibility\n",
    "8. **names** do not\n",
    "9. **upper and lower case spelling** is fun to play with \n",
    "10. **question marks** are uninteresting\n",
    "11. **exclamation marks and inverted commas** are unnecessary and ugly\n",
    "12. **commas** are useless\n",
    "13. the **dot** leads the text to irs own life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "#### 1. Set variables (strings)\n",
    "So let us now read a poem by Gertrude Stein from [»Before the Flowers of Friendship Faded Friendship Faded«](https://www.poetrynook.com/poem/flowers-friendship-faded-friendship-faded), Kapitel XII \n",
    "\n",
    "\n",
    "\n",
    "<pre>\n",
    "I am very hungry when I drink\n",
    "I need to leave it when I have it held,\n",
    "They will be white with which they know they see, that darker makes it be a color white for me, white is not shown when I am dark indeed with red despair who comes who has to care that they will let me a little lie like now I like to lie I like to live I like to die I like to lie and live and die and live and die and by and by I like to live and die and by and by they need to sew, the difference is that sewing makes it bleed and such with them in all the way of seed and seeding and repine and they will which is mine and not all mine who can be thought curious of this of all of that made it and come lead it and done weigh it and mourn and sit upon it know it for ripeness without deserting all of it of which without which it has not been born. Oh no not to be thirsty with the thirst of hunger not alone to know that they plainly and ate or wishes. Any little one will kill himself for milk.\n",
    "</pre>\n",
    "\n",
    "read into the machine and output again *in the same way* with the `print` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am very hungry when I drink\n",
      "I need to leave it when I have it held,\n",
      "They will be white with which they know they see, that darker makes it be a color white for me, white is not shown when I am dark indeed with red despair who comes who has to care that they will let me a little lie like now I like to lie I like to live I like to die I like to lie and live and die and live and die and by and by I like to live and die and by and by they need to sew, the difference is that sewing makes it bleed and such with them in all the way of seed and seeding and repine and they will which is mine and not all mine who can be thought curious of this of all of that made it and come lead it and done weigh it and mourn and sit upon it know it for ripeness without deserting all of it of which without which it has not been born. Oh no not to be thirsty with the thirst of hunger not alone to know that they plainly and ate or wishes. Any little one will kill himself for milk.\n"
     ]
    }
   ],
   "source": [
    "XII = \"\"\"I am very hungry when I drink\n",
    "I need to leave it when I have it held,\n",
    "They will be white with which they know they see, that darker makes it be a color white for me, white is not shown when I am dark indeed with red despair who comes who has to care that they will let me a little lie like now I like to lie I like to live I like to die I like to lie and live and die and live and die and by and by I like to live and die and by and by they need to sew, the difference is that sewing makes it bleed and such with them in all the way of seed and seeding and repine and they will which is mine and not all mine who can be thought curious of this of all of that made it and come lead it and done weigh it and mourn and sit upon it know it for ripeness without deserting all of it of which without which it has not been born. Oh no not to be thirsty with the thirst of hunger not alone to know that they plainly and ate or wishes. Any little one will kill himself for milk.\"\"\"\n",
    "print(XII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import necessary libraries\n",
    "then we import a Python library called `NLTK` (Natural Language Toolkit) and continue working with this library, which was created especially for NLP purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tokenisation & punctuation\n",
    "from NLTK we now import the so-called *Wordtokenizer* `word_tokenize()`, an algorithm that (simply said) breaks the text down into single strings, so-called *tokens* which represent words.\n",
    "\n",
    "Then we filter out all tokens that are not alphabetical. For our case: all independent punctuation.\n",
    "\n",
    "Python has the function `isalpha ()`, which can be used for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'very', 'hungry', 'when', 'I', 'drink', 'I', 'need', 'to', 'leave', 'it', 'when', 'I', 'have', 'it', 'held', 'They', 'will', 'be', 'white', 'with', 'which', 'they', 'know', 'they', 'see', 'that', 'darker', 'makes', 'it', 'be', 'a', 'color', 'white', 'for', 'me', 'white', 'is', 'not', 'shown', 'when', 'I', 'am', 'dark', 'indeed', 'with', 'red', 'despair', 'who', 'comes', 'who', 'has', 'to', 'care', 'that', 'they', 'will', 'let', 'me', 'a', 'little', 'lie', 'like', 'now', 'I', 'like', 'to', 'lie', 'I', 'like', 'to', 'live', 'I', 'like', 'to', 'die', 'I', 'like', 'to', 'lie', 'and', 'live', 'and', 'die', 'and', 'live', 'and', 'die', 'and', 'by', 'and', 'by', 'I', 'like', 'to', 'live', 'and', 'die', 'and', 'by', 'and', 'by', 'they', 'need', 'to', 'sew', 'the', 'difference', 'is', 'that', 'sewing', 'makes', 'it', 'bleed', 'and', 'such', 'with', 'them', 'in', 'all', 'the', 'way', 'of', 'seed', 'and', 'seeding', 'and', 'repine', 'and', 'they', 'will', 'which', 'is', 'mine', 'and', 'not', 'all', 'mine', 'who', 'can', 'be', 'thought', 'curious', 'of', 'this', 'of', 'all', 'of', 'that', 'made', 'it', 'and', 'come', 'lead', 'it', 'and', 'done', 'weigh', 'it', 'and', 'mourn', 'and', 'sit', 'upon', 'it', 'know', 'it', 'for', 'ripeness', 'without', 'deserting', 'all', 'of', 'it', 'of', 'which', 'without', 'which', 'it', 'has', 'not', 'been', 'born', 'Oh', 'no', 'not', 'to', 'be', 'thirsty', 'with', 'the', 'thirst', 'of', 'hunger', 'not', 'alone', 'to', 'know', 'that', 'they', 'plainly', 'and', 'ate', 'or', 'wishes', 'Any', 'little', 'one', 'will', 'kill', 'himself', 'for', 'milk']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized = word_tokenize(XII)\n",
    "# remove all tokens that are not alphabetic\n",
    "tokenized_word = [word for word in tokenized if word.isalpha()]\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Upper and lower case\n",
    "\n",
    "Now we convert all words into one case, in *lower case*.\n",
    "\n",
    "This means that the vocabulary is reduced, but also that some distinctions are lost  (e.g. “Apple” the company v.s. “apple” the fruit is a commonly used example)\n",
    "\n",
    "To do this, we call the function `lower()` for each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "## note4me: when nicht alles in kleingeschrieben dann bleibt beispielsweise das I nach dem lemmatizen und stemmen mit drinnen...\n",
    "tokenized_word = [w.lower() for w in tokenized_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. stop words\n",
    "**Stop words** are regarded as noise in the text. Words like *is, am, are, this, a, an, the* for example.\n",
    "\n",
    "These words are among the most common words in English texts and Intuitively, it seems strange to count these words like \"the\" and \"and\" among the \"most common,\" because words like these are presumably common across all texts, not just this text in particular.\n",
    "\n",
    "To remove stopwords, NLTK requires that we first create a list of stopwords (a list of commonly-occurring English words that shouldn't be counted for the purpose of word frequency.) and then filter this list of tokens out of the text.\n",
    "\n",
    "1. create a list of stopwords: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'after', 'but', 'she', 'o', 'how', 'for', 'their', 'no', 'shouldn', 'did', \"you've\", 've', \"you're\", 'doesn', 'yourselves', 'himself', 'most', 'my', 'of', 'mustn', 'ourselves', 'by', 'such', 'i', 'now', 'if', 'against', 'these', 'off', 'll', 'out', 'yourself', 'ours', 'as', 'with', 'shan', 'isn', 'been', 'that', \"weren't\", 'should', 'than', 'do', 're', \"didn't\", 'over', \"wasn't\", \"isn't\", 'be', 'what', 'they', 'both', \"that'll\", 'them', 'on', \"should've\", 'just', 'down', 'y', 'those', 'm', 'in', 'very', 'didn', 'hadn', 'myself', 'here', 'under', \"mightn't\", 'hers', 'each', 'until', 'can', \"haven't\", 'same', 'needn', 'themselves', 'hasn', 'below', 'above', 'when', 'weren', \"she's\", 'why', 'not', \"needn't\", 'will', 'itself', 'had', \"shan't\", 'don', 'an', 'ain', 'aren', 'his', 'nor', 'too', \"hadn't\", 'wasn', 'so', 'is', 'are', 'between', 'won', 'haven', 'him', 'he', 'being', 'during', 'has', 'this', 'at', 'while', \"you'll\", 'through', 'your', 'up', 'further', \"aren't\", 'some', 'herself', 'whom', 'am', 'to', 'me', 'our', 'her', 'and', 'before', 's', 'having', 'does', 'doing', 'the', 'couldn', 'more', 'about', 'which', \"shouldn't\", 'was', 'once', 'wouldn', 'again', 'into', 'then', 'were', 'have', 'only', 'because', \"don't\", 'a', 'any', 'few', 'we', 'theirs', 't', 'its', 'own', 'mightn', \"won't\", \"couldn't\", 'where', 'd', 'there', 'who', 'ma', \"doesn't\", \"mustn't\", \"you'd\", 'or', 'other', \"hasn't\", 'you', 'all', 'yours', 'from', \"it's\", \"wouldn't\", 'it'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. filter out stopwords: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['i', 'am', 'very', 'hungry', 'when', 'i', 'drink', 'i', 'need', 'to', 'leave', 'it', 'when', 'i', 'have', 'it', 'held', 'they', 'will', 'be', 'white', 'with', 'which', 'they', 'know', 'they', 'see', 'that', 'darker', 'makes', 'it', 'be', 'a', 'color', 'white', 'for', 'me', 'white', 'is', 'not', 'shown', 'when', 'i', 'am', 'dark', 'indeed', 'with', 'red', 'despair', 'who', 'comes', 'who', 'has', 'to', 'care', 'that', 'they', 'will', 'let', 'me', 'a', 'little', 'lie', 'like', 'now', 'i', 'like', 'to', 'lie', 'i', 'like', 'to', 'live', 'i', 'like', 'to', 'die', 'i', 'like', 'to', 'lie', 'and', 'live', 'and', 'die', 'and', 'live', 'and', 'die', 'and', 'by', 'and', 'by', 'i', 'like', 'to', 'live', 'and', 'die', 'and', 'by', 'and', 'by', 'they', 'need', 'to', 'sew', 'the', 'difference', 'is', 'that', 'sewing', 'makes', 'it', 'bleed', 'and', 'such', 'with', 'them', 'in', 'all', 'the', 'way', 'of', 'seed', 'and', 'seeding', 'and', 'repine', 'and', 'they', 'will', 'which', 'is', 'mine', 'and', 'not', 'all', 'mine', 'who', 'can', 'be', 'thought', 'curious', 'of', 'this', 'of', 'all', 'of', 'that', 'made', 'it', 'and', 'come', 'lead', 'it', 'and', 'done', 'weigh', 'it', 'and', 'mourn', 'and', 'sit', 'upon', 'it', 'know', 'it', 'for', 'ripeness', 'without', 'deserting', 'all', 'of', 'it', 'of', 'which', 'without', 'which', 'it', 'has', 'not', 'been', 'born', 'oh', 'no', 'not', 'to', 'be', 'thirsty', 'with', 'the', 'thirst', 'of', 'hunger', 'not', 'alone', 'to', 'know', 'that', 'they', 'plainly', 'and', 'ate', 'or', 'wishes', 'any', 'little', 'one', 'will', 'kill', 'himself', 'for', 'milk'] \n",
      "\n",
      "Filterd Sentence: ['hungry', 'drink', 'need', 'leave', 'held', 'white', 'know', 'see', 'darker', 'makes', 'color', 'white', 'white', 'shown', 'dark', 'indeed', 'red', 'despair', 'comes', 'care', 'let', 'little', 'lie', 'like', 'like', 'lie', 'like', 'live', 'like', 'die', 'like', 'lie', 'live', 'die', 'live', 'die', 'like', 'live', 'die', 'need', 'sew', 'difference', 'sewing', 'makes', 'bleed', 'way', 'seed', 'seeding', 'repine', 'mine', 'mine', 'thought', 'curious', 'made', 'come', 'lead', 'done', 'weigh', 'mourn', 'sit', 'upon', 'know', 'ripeness', 'without', 'deserting', 'without', 'born', 'oh', 'thirsty', 'thirst', 'hunger', 'alone', 'know', 'plainly', 'ate', 'wishes', 'little', 'one', 'kill', 'milk']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent=[]\n",
    "\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "\n",
    "print(\"Tokenized Sentence:\",tokenized_word, \"\\n\")\n",
    "print(\"Filterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 Lemmatization\n",
    "Another important method of text preparation, probably the most common reduction method in NLP is *lemmatization*. It reduces words to their source word, the linguistically correct *lemma*. The word *better*, for example, has *good* as lemma. This means that lemmas, in contrast to the process of *Stemming* (a process of linguistic normalisation), already carry the word context within themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Sentence: ['hungry', 'drink', 'need', 'leave', 'held', 'white', 'know', 'see', 'darker', 'make', 'color', 'white', 'white', 'shown', 'dark', 'indeed', 'red', 'despair', 'come', 'care', 'let', 'little', 'lie', 'like', 'like', 'lie', 'like', 'live', 'like', 'die', 'like', 'lie', 'live', 'die', 'live', 'die', 'like', 'live', 'die', 'need', 'sew', 'difference', 'sewing', 'make', 'bleed', 'way', 'seed', 'seeding', 'repine', 'mine', 'mine', 'thought', 'curious', 'made', 'come', 'lead', 'done', 'weigh', 'mourn', 'sit', 'upon', 'know', 'ripeness', 'without', 'deserting', 'without', 'born', 'oh', 'thirsty', 'thirst', 'hunger', 'alone', 'know', 'plainly', 'ate', 'wish', 'little', 'one', 'kill', 'milk']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#ps = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "lemmatized_words=[]\n",
    "\n",
    "for w in filtered_sent:\n",
    "    lemmatized_words.append(lem.lemmatize(w))\n",
    "\n",
    "#print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Lemmatized Sentence:\",lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. POS tagging\n",
    "this is now our text, which we can read into further NLP-algorithms or to create f.ex. *word embeddings* to generate meanings in the form of vectors.\n",
    "\n",
    "Let us remember the description of Gertrude Stein when she starts talking about verbs, subjectives, nouns etc. This grammatical classification is also necessary in NLP. In our example text, we determine the respective grammatical group according to the *Penn Treebank-Stadards Table* using the *Part-of-Speech(POS) tagging method, which searches for relationships within the sentence and assigns it a corresponding 'tag'. For example, in the common [Penn Treebank-Stadandards Table](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) (link), *VB* stands for the verb base form or *NN* for a noun.\n",
    "\n",
    "We place the initial *tokens* in the POS tagger: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('very', 'RB'),\n",
       " ('hungry', 'JJ'),\n",
       " ('when', 'WRB'),\n",
       " ('i', 'NN'),\n",
       " ('drink', 'VBP'),\n",
       " ('i', 'NNS'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('leave', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('when', 'WRB'),\n",
       " ('i', 'NN'),\n",
       " ('have', 'VBP'),\n",
       " ('it', 'PRP'),\n",
       " ('held', 'VBD'),\n",
       " ('they', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('white', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('they', 'PRP'),\n",
       " ('know', 'VBP'),\n",
       " ('they', 'PRP'),\n",
       " ('see', 'VBP'),\n",
       " ('that', 'IN'),\n",
       " ('darker', 'NN'),\n",
       " ('makes', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('be', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('color', 'NN'),\n",
       " ('white', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('me', 'PRP'),\n",
       " ('white', 'JJ'),\n",
       " ('is', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('shown', 'VBN'),\n",
       " ('when', 'WRB'),\n",
       " ('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('dark', 'JJ'),\n",
       " ('indeed', 'RB'),\n",
       " ('with', 'IN'),\n",
       " ('red', 'JJ'),\n",
       " ('despair', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('comes', 'VBZ'),\n",
       " ('who', 'WP'),\n",
       " ('has', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('care', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('let', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('a', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('lie', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('now', 'RB'),\n",
       " ('i', 'VBP'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('lie', 'VB'),\n",
       " ('i', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('live', 'VB'),\n",
       " ('i', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('die', 'VB'),\n",
       " ('i', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('lie', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('live', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('die', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('live', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('die', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('and', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('i', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('live', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('die', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('and', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('sew', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('difference', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('sewing', 'VBG'),\n",
       " ('makes', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('bleed', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('such', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('them', 'PRP'),\n",
       " ('in', 'IN'),\n",
       " ('all', 'PDT'),\n",
       " ('the', 'DT'),\n",
       " ('way', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('seed', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('seeding', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('repine', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('they', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('which', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('mine', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('not', 'RB'),\n",
       " ('all', 'DT'),\n",
       " ('mine', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('thought', 'VBN'),\n",
       " ('curious', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('made', 'VBD'),\n",
       " ('it', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('come', 'JJ'),\n",
       " ('lead', 'NN'),\n",
       " ('it', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('done', 'VBN'),\n",
       " ('weigh', 'VBP'),\n",
       " ('it', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('mourn', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('sit', 'VB'),\n",
       " ('upon', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('know', 'VBP'),\n",
       " ('it', 'PRP'),\n",
       " ('for', 'IN'),\n",
       " ('ripeness', 'NN'),\n",
       " ('without', 'IN'),\n",
       " ('deserting', 'VBG'),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('of', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('without', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('it', 'PRP'),\n",
       " ('has', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('been', 'VBN'),\n",
       " ('born', 'VBN'),\n",
       " ('oh', 'IN'),\n",
       " ('no', 'DT'),\n",
       " ('not', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('thirsty', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('thirst', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('hunger', 'NN'),\n",
       " ('not', 'RB'),\n",
       " ('alone', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('know', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('plainly', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('ate', 'VB'),\n",
       " ('or', 'CC'),\n",
       " ('wishes', 'VB'),\n",
       " ('any', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('one', 'CD'),\n",
       " ('will', 'MD'),\n",
       " ('kill', 'VB'),\n",
       " ('himself', 'PRP'),\n",
       " ('for', 'IN'),\n",
       " ('milk', 'NN')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokenized_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
