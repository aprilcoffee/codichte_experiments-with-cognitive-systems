{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the Road"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're working with the text of the project [On the Road](https://gregorweichbrodt.de/project/on-the-road.html) by Gregor Weichbrodt.\n",
    "\n",
    "We have a folder of images. Each image (screenshot) contains a part of the text we want to process:<br>\n",
    "- digitize texts with OCR (optical character recognition)\n",
    "- merge all texts into one text\n",
    "- perform text analysis/ synthesis with this corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Translate text from images to text files. \n",
    "This code block should be performed on a local computer, not in this binder environment, because it requires a library (tesseract) outside of python. \n",
    "(As far as I know) we can't install it on binder.\n",
    "If you want to run this code on your local machine, uncomment (delete the #) the two lines which start with an !.\n",
    "'''\n",
    "\n",
    "# create directory for textfiles\n",
    "# !mkdir $PWD/data/on-the-road/txt\n",
    "# translate each image with the library tesseract\n",
    "# uncomment the next line if you need to install the library\n",
    "# !apt install tesseract-ocr\n",
    "# !for f in $PWD/data/on-the-road/screenshots/*.png; do tesseract $f $PWD/data/on-the-road/txt/${f##*/}; done;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' The following bash command (starting with an !) merges all text files into one textfile. \n",
    "This is one way to do it, we will perform it with python below. '''\n",
    "# !cat $PWD/data/on-the-road/txt/*.txt >> $PWD/data/on-the-road/GregorWeichbrodt_On-the-Road.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsorted list:\n",
      "\n",
      "009.png.txt\n",
      "019.png.txt\n",
      "014.png.txt\n",
      "047.png.txt\n",
      "\n",
      "sorted list:\n",
      "\n",
      "001.png.txt\n",
      "002.png.txt\n",
      "003.png.txt\n",
      "004.png.txt\n"
     ]
    }
   ],
   "source": [
    "''' Merge all text files. '''\n",
    "\n",
    "# Store names of text files in one list\n",
    "import os\n",
    "path = 'data/on-the-road/txt/'\n",
    "files = os.listdir(path)\n",
    "\n",
    "# Print first items\n",
    "print('unsorted list:\\n')\n",
    "for i in range(4):\n",
    "    print(files[i])\n",
    "\n",
    "# The list is not sorted. For this case it does not matter, but we will sort it nevertheless.\n",
    "files = sorted(files)\n",
    "print('\\nsorted list:\\n')\n",
    "for i in range(4):\n",
    "    print(files[i])\n",
    "    \n",
    "# Create an empty variable where we will store all texts.\n",
    "txt = ''\n",
    "# Iterate through files and append them.\n",
    "for file in files:\n",
    "    with open(path+file, 'r') as f:\n",
    "        txt += f.read()\n",
    "        txt += '\\n'\n",
    "\n",
    "# Write txt to disk.\n",
    "f = open('data/on-the-road/GregorWeichbrodt_On-the-Road.txt', 'w')\n",
    "f.write(txt)\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load On the Road, created by Gregor Weichbrodt. '''\n",
    "with open('data/on-the-road/GregorWeichbrodt_On-the-Road.txt','r') as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 53075\n",
      "First characters: CHAPTER 1\n",
      "\n",
      "Head northwest on W 47th St toward 7th\n",
      "Ave. Take the 1st left onto 7th Ave. Turn\n",
      "right on\n"
     ]
    }
   ],
   "source": [
    "print('Number of characters:', len(txt))\n",
    "print('First characters:',txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tokens: ['CHAPTER', '1', 'Head', 'northwest', 'on', 'W', '47th', 'St', 'toward', '7th', 'Ave', '.', 'Take', 'the', '1st', 'left', 'onto', '7th', 'Ave', '.', 'Turn', 'right', 'onto', 'W', '39th', 'St', '.', 'Take', 'the', 'ramp', 'onto', 'Lincoln', 'Tunnel', '.', 'Parts', 'of', 'this', 'road', 'are', 'closed', 'Mon', '-', 'Fri', '4', ':', '00', '-', '7', ':', '00', 'pm', '.', 'Entering', 'New', 'Jersey', '.', 'Continue', 'onto', 'NJ', '-', '495', 'W', '.', 'Keep', 'right', 'to', 'continue', 'on', 'NJ', '-', '3', 'W', ',', 'follow', 'signs', 'for', 'New', 'Jersey', '3', 'W', '/', 'Garden', 'State', 'Parkway', '/', 'Secaucus', '.', 'Take', 'the', 'New', 'Jersey', '3', 'W', 'exit', 'on', 'the', 'left', 'toward', 'Clifton', '.']\n"
     ]
    }
   ],
   "source": [
    "''' Tokenize text. '''\n",
    "import nltk\n",
    "tokens = nltk.wordpunct_tokenize(txt)\n",
    "print('First tokens:',tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary: 1095\n",
      "First items of vocabulary: ['!-', '(', ').', ',', '-', '.', '..', './', '/', '00', '1', '10', '100', '101', '101S', '107th', '10th', '11', '113', '115', '119', '11B', '11th', '12', '120', '123B', '125', '126', '12th', '12thStSW', '13', '131', '133', '134th', '137', '138', '139', '13N', '13th', '14', '146', '14th', '15', '151', '151B', '153', '155A', '155P', '157', '15E', '15X', '15th', '160B', '164', '165', '168', '169', '16E', '16th', '170', '170S', '17th', '18', '180', '184', '189', '18E', '18th', '19', '190', '191', '19B', '19th', '1A', '1Alt', '1B', '1C', '1st', '2', '20', '200', '2005S', '202', '205', '206', '208', '209B', '20A', '20BUS', '21', '210', '211', '213', '215', '216A', '22', '22A', '22nd', '23', '231']\n"
     ]
    }
   ],
   "source": [
    "''' Create a vocabulary. '''\n",
    "vocab = sorted(set(tokens))\n",
    "print('Length of vocabulary:', len(vocab))\n",
    "print('First items of vocabulary:', vocab[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Frequency distribution. '''\n",
    "from nltk import FreqDist\n",
    "freq_dist = nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 1415, 'onto': 830, '-': 636, '/': 476, 'St': 400, 'Turn': 380, 'left': 338, 'right': 335, 'the': 318, 'W': 297, ...})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list freq_dist contains all tokens and next to each token its number of appearances.\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 lowest:\n",
      "Secaucus 1\n",
      "Cianci 1\n",
      "509S 1\n",
      "62A 1\n",
      "62B 1\n",
      "Brook 1\n",
      "13N 1\n",
      "Mtn 1\n",
      "Old 1\n",
      "980T 1\n",
      "\n",
      "10 highest:\n",
      ". 1415\n",
      "onto 830\n",
      "- 636\n",
      "/ 476\n",
      "St 400\n",
      "Turn 380\n",
      "left 338\n",
      "right 335\n",
      "the 318\n",
      "W 297\n"
     ]
    }
   ],
   "source": [
    "# Sort list by appearance (low to high). This drops the values next to each word!\n",
    "freq_dist_ascending = sorted(freq_dist, key=freq_dist.get)\n",
    "# High to low:\n",
    "freq_dist_descending = list(reversed(freq_dist_ascending))\n",
    "print('10 lowest:')\n",
    "for i in range(10):\n",
    "    key = freq_dist_ascending[i]\n",
    "    print(key, freq_dist[key])\n",
    "\n",
    "print('\\n10 highest:')\n",
    "for i in range(10):\n",
    "    key = freq_dist_descending[i]\n",
    "    print(key, freq_dist[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 highest:\n",
      "\n",
      "onto 830\n",
      "St 400\n",
      "Turn 380\n",
      "left 338\n",
      "right 335\n",
      "the 318\n",
      "W 297\n",
      "Take 288\n",
      "S 263\n",
      "toward 252\n",
      "E 232\n",
      "to 228\n",
      "on 207\n",
      "Continue 205\n",
      "US 201\n",
      "N 185\n",
      "I 184\n",
      "exit 156\n",
      "Ave 151\n",
      "follow 138\n",
      "for 124\n",
      "Head 118\n",
      "Merge 101\n",
      "merge 100\n",
      "1st 100\n"
     ]
    }
   ],
   "source": [
    "''' Frequency distribution without punctuation. '''\n",
    "# We create a new list which contains only alphabetical and numerical values.\n",
    "freq_dist = nltk.FreqDist([token for token in tokens if token.isalnum()])\n",
    "freq_dist_ascending = sorted(freq_dist, key=freq_dist.get)\n",
    "# High to low:\n",
    "freq_dist_descending = list(reversed(freq_dist_ascending))\n",
    "\n",
    "print('25 highest:\\n')\n",
    "for i in range(25):\n",
    "    key = freq_dist_descending[i]\n",
    "    print(key, freq_dist[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud Generator\n",
    "\n",
    "Here is a visual translation of the frequency distribution. Done with:<br>\n",
    "https://www.jasondavies.com/wordcloud/ \n",
    "\n",
    "![Image](data/on-the-road/wordcloud_250words.svg)<br>\n",
    "(250 words/ numbers)<br><br>\n",
    "![Image](data/on-the-road/wordcloud_1095words.svg)<br>\n",
    "(all words/ numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffled directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to Street left- merge 1st/ 421 Continue Exit left onto Head signs Take Beachwood.- Expy- Turn Civic Hwy roundabout Entering Slight the/ S Pkwy.. ramp. 7th. Entering left 464B- Entering right- Take Head, onto Slight at W on Jersey E. right 6- right Interstate N \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# import textwrap\n",
    "\n",
    "random.shuffle(tokens)\n",
    "# print(textwrap.fill(\" \".join(tokens), 60))\n",
    "\n",
    "# create an empty variable to store our generated text\n",
    "out = ''\n",
    "# append the first n tokens, separated with ' ' (if next token is a word).\n",
    "for i in range(60):\n",
    "    out += tokens[i]\n",
    "    if tokens[i+1].isalnum():\n",
    "        out += ' '\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
